{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS FS Command "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help\n",
    "**Usage: hadoop fs [generic options] -help [cmd ...]<br>**\n",
    "**Return usage output**\n",
    "\n",
    "### usage\n",
    "**Usage: hadoop fs -usage command** <br>\n",
    "Return the help for an individual command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -ls [-d] [-h] [-R] [<path> ...]\r\n"
     ]
    }
   ],
   "source": [
    "##列出指令用法包含參數說明\n",
    "#!hadoop fs -help ls\n",
    "\n",
    "##列出指令的用法,但是不包含說明\n",
    "!hadoop fs -usage ls\n",
    "\n",
    "##列出fs相關指令的所有用法\n",
    "#!hadoop fs -usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ls\n",
    "**Usage: hadoop fs -ls [-d] [-h] [-R]  &lt;args&gt;**\n",
    "\n",
    "Options:\n",
    "```\n",
    "-d: Directories are listed as plain files.\n",
    "-h: Format file sizes in a human-readable fashion (eg 64.0m instead of 67108864).\n",
    "-R: Recursively list subdirectories encountered.\n",
    "```\n",
    "\n",
    "### lsr\n",
    "\n",
    "**Usage: hadoop fs -lsr &lt;args&gt;**<br>\n",
    "Recursive version of ls.<br>\n",
    "Note: This command is deprecated. Instead use hadoop fs -ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -ls [-d] [-h] [-R] [<path> ...]\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-23 16:24 /tmp/examples\n",
      "drwxrwx---   - hadoop supergroup          0 2017-07-27 16:01 /tmp/hadoop-yarn\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-23 16:39 /tmp/history\n",
      "drwx-wx-wx   - hadoop supergroup          0 2017-07-27 15:57 /tmp/hive\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-22 17:43 /tmp/sample_table\n",
      "\n",
      "-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:43 /tmp/dataframe_sample.csv\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-22 16:50 /tmp/test1.csv\n",
      "-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:11 /tmp/test2.csv\n",
      "\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-07-27 16:34 /user/hadoop/workflow/spark-import/lib\n",
      "-rw-r--r--   2 hadoop supergroup      71181 2017-07-27 16:34 /user/hadoop/workflow/spark-import/lib/MSGImport.jar\n",
      "-rw-r--r--   2 hadoop supergroup       1059 2017-07-27 16:34 /user/hadoop/workflow/spark-import/workflow.xml\n",
      "\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-23 16:24 /tmp\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage ls\n",
    "##如果只想用ls指令查詢目錄列表,可以使用grep搭配列出\n",
    "!hadoop fs -ls /tmp | grep \"drw\"\n",
    "print()\n",
    "##如果只想用ls指令查詢某種檔案類型,可以使用grep搭配列出\n",
    "!hadoop fs -ls /tmp | grep \".csv\"\n",
    "print()\n",
    "##列出某個目錄下(包含子目錄)所有檔案及目錄名稱\n",
    "!hadoop fs -ls -R /user/hadoop/workflow/spark-import\n",
    "print()\n",
    "##僅列出指定的目錄名稱\n",
    "!hadoop fs -ls -d -R /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mkdir\n",
    "**Usage: hadoop fs -mkdir [-p] &lt;paths&gt;**<br>\n",
    "Takes path uri’s as argument and creates directories.\n",
    "\n",
    "<p>\n",
    "Options:<br>\n",
    "```\n",
    "The -p option behavior is much like Unix mkdir -p, creating parent directories along the path.\n",
    "```\n",
    "<p>\n",
    "Example:<br>\n",
    "```\n",
    "hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2\n",
    "hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -mkdir [-p] <path> ...\n",
      "Found 11 items\n",
      "-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:43 /tmp/dataframe_sample.csv\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-23 16:24 /tmp/examples\n",
      "drwxrwx---   - hadoop supergroup          0 2017-07-27 16:01 /tmp/hadoop-yarn\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-23 16:39 /tmp/history\n",
      "drwx-wx-wx   - hadoop supergroup          0 2017-07-27 15:57 /tmp/hive\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-22 17:43 /tmp/sample_table\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-22 16:31 /tmp/test1._COPYING_\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-22 16:50 /tmp/test1.csv\n",
      "-rw-r--r--   2 hadoop supergroup         10 2017-11-22 16:42 /tmp/test1.tx\n",
      "-rw-r--r--   2 hadoop supergroup         24 2017-11-22 16:41 /tmp/test1.txt\n",
      "-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:11 /tmp/test2.csv\n",
      "Found 11 items\n",
      "-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:43 /tmp/dataframe_sample.csv\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-23 16:40 /tmp/examples\n",
      "drwxrwx---   - hadoop supergroup          0 2017-07-27 16:01 /tmp/hadoop-yarn\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-23 16:39 /tmp/history\n",
      "drwx-wx-wx   - hadoop supergroup          0 2017-07-27 15:57 /tmp/hive\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-22 17:43 /tmp/sample_table\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-22 16:31 /tmp/test1._COPYING_\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-22 16:50 /tmp/test1.csv\n",
      "-rw-r--r--   2 hadoop supergroup         10 2017-11-22 16:42 /tmp/test1.tx\n",
      "-rw-r--r--   2 hadoop supergroup         24 2017-11-22 16:41 /tmp/test1.txt\n",
      "-rw-r--r--   2 hadoop supergroup        272 2017-11-22 17:11 /tmp/test2.csv\n",
      "17/11/23 16:40:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp/examples\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage mkdir\n",
    "!hadoop fs -ls  /tmp\n",
    "\n",
    "##原本如果該目錄不存在,加入\"-p\"指令系統會自動連上層目錄一起建立\n",
    "!hadoop fs -mkdir -p /tmp/examples/test/test1\n",
    "\n",
    "!hadoop fs -ls /tmp\n",
    "\n",
    "##刪除目錄,會連同目錄下所有目錄與檔案一併刪除\n",
    "!hadoop fs -rm -r /tmp/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put\n",
    "\n",
    "**Usage: hadoop fs -put [-f] [-p] [-l]  [ - | &lt;localsrc1&gt; .. ]. &lt;dst&gt;**\n",
    "\n",
    "```\n",
    "Options:\n",
    "-p : Preserves access and modification times, ownership and the permissions.\n",
    "-f : Overwrites the destination if it already exists.\n",
    "-l : Allow DataNode to lazily persist the file to disk, Forces a replication factor of 1. \n",
    "     This flag will result in reduced durability. Use with care.\n",
    "```\n",
    "Example1:<br>\n",
    "```\n",
    "hadoop fs -put localfile /user/hadoop/hadoopfile\n",
    "hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir\n",
    "hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile\n",
    "hadoop fs -put - hdfs://host:port/hadoop/hadoopfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -put [-f] [-p] [-l] <localsrc> ... <dst>\n",
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop supergroup         28 2017-11-23 17:39 /tmp/examples/hdfs-example1.txt\n",
      "-rw-r--r--   2 hadoop supergroup          4 2017-11-23 17:39 /tmp/examples/hdfs-example2.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage put\n",
    "!mkdir -p ~/data\n",
    "!echo \"ttt\" > ~/data/hdfs-example1.txt\n",
    "!echo \"ggg\" >> ~/data/hdfs-example1.txt\n",
    "!echo \"aaa\" >> ~/data/hdfs-example1.txt\n",
    "!echo \"bbb\" >> ~/data/hdfs-example1.txt\n",
    "!echo \"ccc\" >> ~/data/hdfs-example1.txt\n",
    "!echo \"ddd\" >> ~/data/hdfs-example1.txt\n",
    "!echo \"eee\" >> ~/data/hdfs-example1.txt\n",
    "!echo \"ccc\" > ~/data/hdfs-example2.txt\n",
    "\n",
    "##建立目錄\n",
    "!hadoop fs -mkdir -p /tmp/examples\n",
    "\n",
    "###上傳單一檔案至HDFS,如果檔案存在的話會將檔案overwrite\n",
    "!hadoop fs -put -f ~/data/hdfs-example1.txt /tmp/examples/hdfs-example1.txt\n",
    "\n",
    "###上傳多個檔案至HDFS,如果檔案存在的話會將檔案overwrite\n",
    "!hadoop fs -put -f ~/data/hdfs-example1.txt ~/data/hdfs-example2.txt /tmp/examples\n",
    "\n",
    "!hadoop fs -ls  /tmp/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -cat [-ignoreCrc] <src> ...\n",
      "ttt\n",
      "ggg\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage cat\n",
    "\n",
    "##cat 搭配head可顯示檔案的前幾行\n",
    "!hadoop fs -cat /tmp/examples/hdfs-example1.txt | head -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -tail [-f] <file>\n",
      "ttt\n",
      "ggg\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage tail\n",
    "\n",
    "##-f會列印出最後1K資料,如果搭配head可以印出指定的行數\n",
    "!hadoop fs -tail /tmp/examples/hdfs-example1.txt | head -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>\n",
      "/home/hadoop/data/tmp:\n",
      "total 8\n",
      "drwxrwxr-x. 2 hadoop hadoop 4096 Nov 23 16:56 examples\n",
      "-rw-r--r--. 1 hadoop hadoop    4 Nov 23 16:55 hdfs_sample1.txt\n",
      "\n",
      "/home/hadoop/data/tmp/examples:\n",
      "total 8\n",
      "-rw-r--r--. 1 hadoop hadoop 4 Nov 23 16:56 hdfs-example1.txt\n",
      "-rw-r--r--. 1 hadoop hadoop 4 Nov 23 16:56 hdfs-example2.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage get\n",
    "\n",
    "##將檔案從HDFS下載至Local\n",
    "!mkdir -p ~/data/tmp\n",
    "!hadoop fs -get  /tmp/examples/hdfs-example2.txt ~/data/tmp/hdfs_sample1.txt\n",
    "\n",
    "##將目錄從HDFS下載至Local\n",
    "!hadoop fs -get  /tmp/examples ~/data/tmp\n",
    "\n",
    "!ls -lAR ~/data/tmp\n",
    "\n",
    "!rm -rf ~/data/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -touchz <path> ...\n",
      "Found 4 items\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-23 18:06 /tmp/examples/.txt\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-23 18:12 /tmp/examples/20171123181224\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-23 18:13 /tmp/examples/20171123181341\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-23 18:14 /tmp/examples/20171123181359\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage touchz\n",
    "\n",
    "now = !date +\"%Y%m%d%H%M%S\"\n",
    "datestr = now[0]\n",
    "\n",
    "##建立空白檔案,並以目前時間來做為檔案名稱\n",
    "!hadoop fs -touchz /tmp/examples/$datestr\n",
    "\n",
    "!hadoop fs -ls /tmp/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -test -[defsz] <path>\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage test\n",
    "\n",
    "!hadoop fs -test -f /tmp/examples\n",
    "!hadoop fs -test -d /tmp/examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -du [-s] [-h] <path> ...\n",
      "2.3 G    hdfs://mna1:8020/user/hive/warehouse/arcgis.db\n",
      "2.1 G    hdfs://mna1:8020/user/hive/warehouse/cas_analysis.db\n",
      "73.5 M   hdfs://mna1:8020/user/hive/warehouse/cas_init.db\n",
      "24.7 M   hdfs://mna1:8020/user/hive/warehouse/cas_source.db\n",
      "3.4 K    hdfs://mna1:8020/user/hive/warehouse/emp\n",
      "4.2 K    hdfs://mna1:8020/user/hive/warehouse/emp1\n",
      "278.4 K  hdfs://mna1:8020/user/hive/warehouse/employee\n",
      "5.2 K    hdfs://mna1:8020/user/hive/warehouse/empresult\n",
      "232.4 M  hdfs://mna1:8020/user/hive/warehouse/msg.db\n",
      "272      hdfs://mna1:8020/user/hive/warehouse/page_view_3\n",
      "216      hdfs://mna1:8020/user/hive/warehouse/page_view_3_1\n",
      "0        hdfs://mna1:8020/user/hive/warehouse/page_view_3_2\n",
      "476      hdfs://mna1:8020/user/hive/warehouse/page_view_4\n",
      "468      hdfs://mna1:8020/user/hive/warehouse/page_view_5\n",
      "495      hdfs://mna1:8020/user/hive/warehouse/page_view_6\n",
      "1.1 K    hdfs://mna1:8020/user/hive/warehouse/page_view_7\n",
      "272      hdfs://mna1:8020/user/hive/warehouse/pv_7\n",
      "20       hdfs://mna1:8020/user/hive/warehouse/table_1\n",
      "8        hdfs://mna1:8020/user/hive/warehouse/table_2\n",
      "3.2 M    hdfs://mna1:8020/user/hive/warehouse/taiwan_zip32\n",
      "0        hdfs://mna1:8020/user/hive/warehouse/test2\n",
      "4.8 G  /user/hive\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage du\n",
    "\n",
    "##列出目錄下所有個別目錄及檔案的大小\n",
    "!hadoop fs -du -h hdfs://mna1:8020/user/hive/*\n",
    "\n",
    "##列出目錄大小\n",
    "!hadoop fs -du -s -h /user/hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -df [-h] [<path> ...]\n",
      "Filesystem         Size    Used  Available  Use%\n",
      "hdfs://mna1:8020  2.0 T  25.7 G      1.9 T    1%\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage df\n",
    "\n",
    "##列出目錄下所有檔案的大小\n",
    "!hadoop fs -df -h hdfs://mna1:8020/user/hive/*\n",
    "##!hadoop fs -df -h /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -rm [-f] [-r|-R] [-skipTrash] <src> ...\n",
      "17/11/23 17:55:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp/examples\n",
      "rm: `/tmp/examples/dir1': Is a directory\n",
      "rmdir: `/tmp/examples/dir1': Directory is not empty\n",
      "17/11/23 17:56:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp/examples/dir1\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -usage rm\n",
    "##刪除目錄下所有檔案結構\n",
    "!hadoop fs -rm -r /tmp/examples\n",
    "\n",
    "!hadoop fs -mkdir -p /tmp/examples/dir1\n",
    "!hadoop fs -put -f ~/data/hdfs-example1.txt /tmp/examples/dir1/hdfs-example1.txt\n",
    "\n",
    "##rm只能刪除檔案\n",
    "!hadoop fs -rm /tmp/examples/dir1\n",
    "\n",
    "##rmdir只能刪除目錄\n",
    "!hadoop fs -rmdir /tmp/examples/dir1\n",
    "\n",
    "##rm -r 目錄與檔案都可刪使用時要特別小心\n",
    "!hadoop fs -rm -r /tmp/examples/dir1\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
