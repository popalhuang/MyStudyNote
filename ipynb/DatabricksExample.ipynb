{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n"
     ]
    }
   ],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print (department1)\n",
    "print (employee2)\n",
    "print (departmentWithEmployees1.employees[0].email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]\n"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]\n"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = sqlContext.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Working with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]\n"
     ]
    }
   ],
   "source": [
    "unionDF = df1.unionAll(df2)\n",
    "print(unionDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/11/22 17:42:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/tmp/databricks-df-example.parquet': No such file or directory\n",
      "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r \"/tmp/databricks-df-example.parquet\"\n",
    "unionDF.write.parquet(\"/tmp/databricks-df-example.parquet\")\n",
    "parquetDF = sqlContext.read.parquet(\"/tmp/databricks-df-example.parquet\")\n",
    "print(parquetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(anInt=1), Row(anInt=2), Row(anInt=3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import split, explode\n",
    "eDF = sqlContext.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  a|    b|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = parquetDF.select(explode(\"employees\").alias(\"e\"))\n",
    "explodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "explodeDF.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "# Use `|` instead of `or`\n",
    "filterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "whereDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonNullDF = explodeDF.fillna(\"--\")\n",
    "nonNullDF.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterNonNullDF = explodeDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\n",
    "filterNonNullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------------+\n",
      "|firstName|lastName|count(DISTINCT firstName)|\n",
      "+---------+--------+-------------------------+\n",
      "|     null| wendell|                        0|\n",
      "|    matei|    null|                        1|\n",
      "| xiangrui|    meng|                        1|\n",
      "|  michael|armbrust|                        1|\n",
      "+---------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "countDistinctDF = explodeDF.select(\"firstName\", \"lastName\")\\\n",
    "    .groupBy(\"firstName\", \"lastName\")\\\n",
    "    .agg(countDistinct(\"firstName\"))\n",
    "countDistinctDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[firstName#58, lastName#59], functions=[count(distinct firstName#58)])\n",
      "+- *HashAggregate(keys=[firstName#58, lastName#59], functions=[partial_count(distinct firstName#58)])\n",
      "   +- *HashAggregate(keys=[firstName#58, lastName#59, firstName#58], functions=[])\n",
      "      +- Exchange hashpartitioning(firstName#58, lastName#59, firstName#58, 200)\n",
      "         +- *HashAggregate(keys=[firstName#58, lastName#59, firstName#58], functions=[])\n",
      "            +- *Project [e#48.firstName AS firstName#58, e#48.lastName AS lastName#59]\n",
      "               +- Generate explode(employees#18), false, false, [e#48]\n",
      "                  +- *Scan parquet [employees#18] Format: ParquetFormat, InputPaths: hdfs://mna1:8020/tmp/databricks-df-example.parquet, PushedFilters: [], ReadSchema: struct<employees:array<struct<firstName:string,lastName:string,email:string,salary:bigint>>>\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[firstName#58, lastName#59], functions=[count(distinct firstName#58)])\n",
      "+- *HashAggregate(keys=[firstName#58, lastName#59], functions=[partial_count(distinct firstName#58)])\n",
      "   +- *HashAggregate(keys=[firstName#58, lastName#59, firstName#58], functions=[])\n",
      "      +- Exchange hashpartitioning(firstName#58, lastName#59, firstName#58, 200)\n",
      "         +- *HashAggregate(keys=[firstName#58, lastName#59, firstName#58], functions=[])\n",
      "            +- *Project [e#48.firstName AS firstName#58, e#48.lastName AS lastName#59]\n",
      "               +- Generate explode(employees#18), false, false, [e#48]\n",
      "                  +- *Scan parquet [employees#18] Format: ParquetFormat, InputPaths: hdfs://mna1:8020/tmp/databricks-df-example.parquet, PushedFilters: [], ReadSchema: struct<employees:array<struct<firstName:string,lastName:string,email:string,salary:bigint>>>\n"
     ]
    }
   ],
   "source": [
    "# register the DataFrame as a temp table so that we can query it using SQL\n",
    "explodeDF.registerTempTable(\"databricks_df_example\")\n",
    "\n",
    "# Perform the same query as the DataFrame above and return ``explain``\n",
    "countDistinctDF_sql = sqlContext.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM databricks_df_example GROUP BY firstName, lastName\")\n",
    "\n",
    "countDistinctDF_sql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|    1040000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salarySumDF = explodeDF.agg({\"salary\" : \"sum\"})\n",
    "salarySumDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(explodeDF.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|                 8|\n",
      "|   mean|          130000.0|\n",
      "| stddev|23904.572186687874|\n",
      "|    min|            100000|\n",
      "|    max|            160000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodeDF.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example using Pandas & Matplotlib Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>email</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michael</td>\n",
       "      <td>armbrust</td>\n",
       "      <td>no-reply@berkeley.edu</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--</td>\n",
       "      <td>wendell</td>\n",
       "      <td>no-reply@berkeley.edu</td>\n",
       "      <td>160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michael</td>\n",
       "      <td>armbrust</td>\n",
       "      <td>no-reply@berkeley.edu</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xiangrui</td>\n",
       "      <td>meng</td>\n",
       "      <td>no-reply@stanford.edu</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matei</td>\n",
       "      <td>--</td>\n",
       "      <td>no-reply@waterloo.edu</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>--</td>\n",
       "      <td>wendell</td>\n",
       "      <td>no-reply@berkeley.edu</td>\n",
       "      <td>160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xiangrui</td>\n",
       "      <td>meng</td>\n",
       "      <td>no-reply@stanford.edu</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>matei</td>\n",
       "      <td>--</td>\n",
       "      <td>no-reply@waterloo.edu</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  firstName  lastName                  email  salary\n",
       "0   michael  armbrust  no-reply@berkeley.edu  100000\n",
       "1        --   wendell  no-reply@berkeley.edu  160000\n",
       "2   michael  armbrust  no-reply@berkeley.edu  100000\n",
       "3  xiangrui      meng  no-reply@stanford.edu  120000\n",
       "4     matei        --  no-reply@waterloo.edu  140000\n",
       "5        --   wendell  no-reply@berkeley.edu  160000\n",
       "6  xiangrui      meng  no-reply@stanford.edu  120000\n",
       "7     matei        --  no-reply@waterloo.edu  140000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf()\n",
    "pdDF = nonNullDF.toPandas()\n",
    "pdDF.plot(x='firstName', y='salary', kind='bar', rot=45)\n",
    "pdDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/11/22 17:42:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/11/22 17:42:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp/databricks-df-example.parquet\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r \"/tmp/databricks-df-example.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame FAQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/11/22 17:43:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/11/22 17:43:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp/dataframe_sample.csv\n",
      "17/11/22 17:43:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "+---+-------------------+-------------------+--------+\n",
      "| id|           end_date|         start_date|location|\n",
      "+---+-------------------+-------------------+--------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|\n",
      "+---+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r \"/tmp/dataframe_sample.csv\"\n",
    "!mkdir -p ~/data\n",
    "\n",
    "!echo id,end_date,start_date,location >  ~/data/dataframe_sample.csv\n",
    "!echo 1,2015-10-14 00:00:00,2015-09-14 00:00:00,CA-SF >>  ~/data/dataframe_sample.csv\n",
    "!echo 2,2015-10-15 01:00:20,2015-08-14 00:00:00,CA-SD >>  ~/data/dataframe_sample.csv\n",
    "!echo 3,2015-10-16 02:30:00,2015-01-14 00:00:00,NY-NY >>  ~/data/dataframe_sample.csv\n",
    "!echo 4,2015-10-17 03:00:20,2015-02-14 00:00:00,NY-NY >>  ~/data/dataframe_sample.csv\n",
    "!echo 5,2015-10-18 04:30:00,2014-04-14 00:00:00,CA-SD >>  ~/data/dataframe_sample.csv\n",
    "\n",
    "!hadoop fs -put ~/data/dataframe_sample.csv /tmp/dataframe_sample.csv\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "formatPackage = \"csv\"\n",
    "df = sqlContext.read.format(formatPackage).options(header='true', delimiter = ',').load(\"/tmp/dataframe_sample.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+\n",
      "| id|           end_date|         start_date|location|      date|\n",
      "+---+-------------------+-------------------+--------+----------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|\n",
      "+---+-------------------+-------------------+--------+----------+\n",
      "\n",
      "+---+-------------------+-------------------+--------+----------+----------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|\n",
      "+---+-------------------+-------------------+--------+----------+----------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|\n",
      "+---+-------------------+-------------------+--------+----------+----------+\n",
      "\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|  NY|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|  NY|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|  SD|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+\n",
      "\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|  NY|      275|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|  NY|      245|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|  SD|      552|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instead of registering a UDF, call the builtin functions to perform operations on the columns.\n",
    "# This will provide a performance improvement as the builtins compile and run in the platform's JVM.\n",
    "\n",
    "# Convert to a Date type\n",
    "df = df.withColumn('date', F.to_date(df.end_date))\n",
    "df.show()\n",
    "\n",
    "# Parse out the date only\n",
    "df = df.withColumn('date_only', F.regexp_replace(df.end_date,' (\\d+)[:](\\d+)[:](\\d+).*$', ''))\n",
    "df.show()\n",
    "\n",
    "# Split a string and index a field\n",
    "df = df.withColumn('city', F.split(df.location, '-')[1])\n",
    "df.show()\n",
    "\n",
    "# Perform a date diff function\n",
    "df = df.withColumn('date_diff', F.datediff(F.to_date(df.end_date), F.to_date(df.start_date)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.registerTempTable(\"sample_df\")\n",
    "spark.sql(\"select * from sample_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"id\":\"1\",\"end_date\":\"2015-10-14 00:00:00\",\"start_date\":\"2015-09-14 00:00:00\",\"location\":\"CA-SF\",\"date\":\"2015-10-14\",\"date_only\":\"2015-10-14\",\"city\":\"SF\",\"date_diff\":30}',\n",
       " '{\"id\":\"2\",\"end_date\":\"2015-10-15 01:00:20\",\"start_date\":\"2015-08-14 00:00:00\",\"location\":\"CA-SD\",\"date\":\"2015-10-15\",\"date_only\":\"2015-10-15\",\"city\":\"SD\",\"date_diff\":62}']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_json = df.toJSON()\n",
    "rdd_json.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|     1001|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|     1002|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|  NY|      275|     1003|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|  NY|      245|     1004|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|  SD|      552|     1005|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "add_n = F.udf(lambda x, y: x + y, IntegerType())\n",
    "\n",
    "# We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type.\n",
    "df = df.withColumn('id_offset', add_n(F.lit(1000), df.id.cast(IntegerType())))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|     1001|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|     1002|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any constants used by UDF will automatically pass through to workers\n",
    "N = 90\n",
    "last_n_days = F.udf(lambda x: x < N, BooleanType())\n",
    "\n",
    "df_filtered = df.filter(last_n_days(df.date_diff))\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|  NY|      275|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|  NY|      245|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|  SD|      552|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_1 = df.createOrReplaceTempView(\"sample_df\")\n",
    "#df_1.show()\n",
    "df_2 = spark.sql(\"select * from sample_df\")\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.clearCache()\n",
    "sqlContext.cacheTable(\"sample_df\")\n",
    "sqlContext.uncacheTable(\"sample_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+--------------+\n",
      "|location|min(id)|count(id)|avg(date_diff)|\n",
      "+--------+-------+---------+--------------+\n",
      "|   NY-NY|      3|        2|         260.0|\n",
      "|   CA-SF|      1|        1|          30.0|\n",
      "|   CA-SD|      2|        2|         307.0|\n",
      "+--------+-------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = df.groupBy(\"location\").agg(F.min(\"id\"), F.count(\"id\"), F.avg(\"date_diff\"))\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/11/22 17:43:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/11/22 17:43:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp/sample_table\n",
      "17/11/22 17:43:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop supergroup          0 2017-11-22 17:43 /tmp/sample_table/_SUCCESS\n",
      "drwxr-xr-x   - hadoop supergroup          0 2017-11-22 17:43 /tmp/sample_table/end_year=2015\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r \"/tmp/sample_table\"\n",
    "df = df.withColumn('end_month', F.month('end_date'))\n",
    "df = df.withColumn('end_year', F.year('end_date'))\n",
    "df.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\n",
    "!hadoop fs -ls \"/tmp/sample_table\"\n",
    "#display(dbutils.fs.ls(\"/tmp/sample_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|test|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_item_schema = StructType([StructField(\"col1\", StringType(), True),\n",
    "                               StructField(\"col2\", IntegerType(), True)])\n",
    "null_df = sqlContext.createDataFrame([(\"test\", 1), (None, 2)], null_item_schema)\n",
    "null_df.filter(\"col1 IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
