HDFS Format之後Phoenix重建步驟
------------------------------
1. stop all service
$HBASE_HOME/bin/stop-hbase.sh
stop-yarn.sh
stop-dfs.sh

2. rm -rf /hadoop/tmp
rm -rf /usr/local/hadoop-2.7.1/tmp
ssh hadoop-slave1 'rm -rf /usr/local/hadoop-2.7.1/tmp'
ssh hadoop-slave2 'rm -rf /usr/local/hadoop-2.7.1/tmp'
ssh hadoop-slave3 'rm -rf /usr/local/hadoop-2.7.1/tmp'
ssh hadoop-slave4 'rm -rf /usr/local/hadoop-2.7.1/tmp'

3.hdfs format:
hadoop namenode -format

4.start all service
start-dfs.sh
start-yarn.sh
$HBASE_HOME/bin/start-hbase.sh

5.cleanZk
$HBASE_HOME/bin/hbase-daemons.sh stop regionserver
$HBASE_HOME/bin/hbase-daemon.sh stop master
$HBASE_HOME/bin/hbase clean --cleanZk
$HBASE_HOME/bin/stop-hbase.sh
$HBASE_HOME/bin/start-hbase.sh


BigData系統架構下相關服務的啟動與停止
----------------------------
/usr/local/hadoop/sbin/stop-yarn|start-yarn       啟動或停止YARN-CLUSTER服務                          
/usr/local/hadoop/sbin/stop-dfs|start-dfs         啟動或停止HDFS服務
/usr/local/hadoop/sbin/start-all|stop-all         啟動或停止所有相關Hadoop服務(因無服務啟動或停止順序性,所以少用此組命令來啟動或停止服務)
mr-jobhistory-daemon.sh start|stop historyserver  啟動或停止JobHistoryServer服務(檔案)
/usr/local/hbase/bin/start-hbase.sh|stop-hbase.sh 啟動或停止HBASE服務
/usr/local/oozie/bin/oozied.sh start|start        啟動或停止oozie服務
/usr/local/phoenix/bin/sqlline.py localhost       啟動Phoenix命令介面
/usr/local/spark/bin/pyspark                      啟動spark-shell命令介面(for Python)
/usr/local/spark/bin/spark-shell                  啟動spark-shell命令介面(for Scala)
/usr/local/spark/bin/spark-submit                 啟動spark-submit介面,後需接相關的參數資料

hadoop-daemon.sh start|stop datanode		  啟動單台的datanode	
yarn-daemon.sh start|stop nodemanager	 	  啟動單台的nomanager


Hbase相關command
$HBASE_HOME/bin/start-hbase.sh			  		啟動Hbase所有相關服務(zookeeper/regionserver)	
$HBASE_HOME/bin/stop-hbase.sh			  		關閉Hbase所有相關服務(zookeeper/regionserver)
$HBASE_HOME/bin/hbase-daemons.sh start|stop regionserver  	啟動或關閉所有regionserver
$HBASE_HOME/bin/hbase-daemons.sh start|stop zookeeper  		啟動或關閉所有zookeeper
$HBASE_HOME/bin/hbase-daemon.sh start|stop regionserver  	啟動或關閉單個regionserver
$HBASE_HOME/bin/hbase-daemon.sh start|stop zookeeper  		啟動或關閉單個zookeeper
$HBASE_HOME/bin/hbase-daemon.sh start|stop master  		啟動或關閉master(HMaster)
$HBASE_HOME/bin/hbase clean --cleanZk				清除zookeeper所有在Hbase中的設定
$HBASE_HOME/bin/hbase shell					進入HBase Shell

Hadoop fs 常用用法整理
-----------------------------
hadoop fs -mkdir -p /user/master   //【遞迴】新建資料夾

hadoop fs -touchz /user/master/test.txt   //新建空文件檔案
hadoop fs -put -f localfile1 localfile2 /user/master   //上傳文件到路徑(-f:覆蓋)
hadoop fs -get /user/master/test1 /user/master/test2 .   //下載文件到路徑 
hadoop fs -copyFromLocal [-f] localfile1 /user/master/file1   //【覆蓋】複製本地文件到HDFS
hadoop fs -appendToFile localfile1 localfile2 /user/master/test   //追加到文件（可以多個本地輸入，最後一個参數是目標文件）

hadoop fs -ls [-R] [-h] [-S] [-r] /user   //【递归】【人类可识别】【按文件大小排序】【反向排序】列出指定目录下的文件（夹）

hadoop fs -cat /user/master/test.txt   //列出文件内容
hadoop fs -text /user/master/test   //列出文件内容（若是压缩文件先解压缩）
hadoop fs -tail /user/master/log   //显示文件最后1KB本地
hadoop fs -tail -f /user/master/log   //显示动态文件最后1KB

hadoop fs -rm [-R] /user/master/test.txt   //【递归】删除文件

hadoop fs -test -e PATH   //PATH是否存在(exist)
hadoop fs -test -d PATH   //是否是目录（directory）
hadoop fs -test -f PATH   //是否是文件（file）
hadoop fs -test -s PATH   //路径下是否为空
hadoop fs -test -z PATH   //文件是否为空（zero）

hadoop fs -count [-h] //【人类可识别】计算目标路径下（包含子路径）文件夹个数，文件个数，总大小
hadoop fs -df [-h] /user/master   //【人类可识别】列出HDFS空间使用情况
hadoop fs -du [-s] [-h] /user   //【summary】【人类可识别】列出目标路径下文件（夹）大小

hadoop fs -chgrp [-R] /user/master   //【遞迴】更改用户组
hadoop fs -chmod [-R] 777 /user/master   //【遞迴】更改文件(夹)权限
hadoop fs -chown [-R] own[:grp] /user/master   //【遞迴】修改文件（夹）所有者

hadoop fs -help ls   //查看命令help文檔


hadoop fs -setrep -R -w 1 /path/to/other   //修改某個目錄下所有檔案的replication factor


YARN 觀察Spark程式的相關的指令
---------------------------------------
yarn application --list
yarn application -kill <id>


oozie命令整理
----------------------------------------
./oozie job -oozie   http://localhost:11000/oozie -config job.properties -run
./oozie job -oozie   http://localhost:11000/oozie -resume 14-20090525161321-oozie-joe
./oozie job -oozie   http://localhost:11000/oozie -suspend 14-20090525161321-oozie-joe
./oozie job -oozie   http://localhost:11000/oozie -kill 14-20090525161321-oozie-joe
./oozie job -oozie   http://localhost:11000/oozie -info 14-20090525161321-oozie-joe
./oozie job -oozie   http://localhost:11000/oozie -localtime -len 2 -filter status=RUNNING|SUCCEEDED|KILLED
./oozie admin -oozie http://localhost:11000/oozie -status
./oozie admin -oozie http://localhost:11000/oozie -version
./oozie admin -oozie http://localhost:11000/oozie -servers
./oozie admin -oozie http://localhost:11000/oozie -shareliblist


WebUI
------------------------------------------
http://hadoop-master:11000/oozie 	##oozie Manager WebUI
http://hadoop-master:8088		##hadoop cluster WebUI
http://hadoop-master:50070		##HDFS Browser WebUI
http://hadoop-master:16010		##HBase WebUI


hive -f message_result.hql  <-- 直接使用hive命令執行HQL檔案中的語法
nohup ./startMSGImportSpark.sh &

Porblem
-----------------------
1.使用OOZIE執行SparkSubmit時,須注意HBase/Phoenix與Hive Library的衝突



